{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53e6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb346a6",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b214a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15440\\1892946032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# import modelling libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m \u001b[1;31m#features built in support for handling categorical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m \u001b[1;31m#designed to be fast and efficient: for both classification and regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m \u001b[1;31m#creates multiple decision trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd #library for data manipulation\n",
    "import numpy as np # library for working with arrays\n",
    "\n",
    "# import preprocessing libraries\n",
    "\n",
    "# creating visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "rcParams['figure.figsize'] = 8,8\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# import modelling libraries\n",
    "from catboost import CatBoostRegressor #features built in support for handling categorical features\n",
    "from xgboost import XGBRegressor #designed to be fast and efficient: for both classification and regression\n",
    "from sklearn.ensemble import RandomForestRegressor #creates multiple decision trees\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "\n",
    "# pd.options.display.max_rows=2000\n",
    "pd.set_option('display.max_rows', 100) #sets maximum rows to 100\n",
    "pd.set_option('display.max_columns', 30) #sets maximum columns to 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477a80a",
   "metadata": {},
   "source": [
    "# 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data\n",
    "df = pd.read_csv('wfp_food_prices_ken.csv', parse_dates=True, index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sample data/records\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c39fa",
   "metadata": {},
   "source": [
    "# 3. Data Type description and statistical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e18652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754caff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check information of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c6665",
   "metadata": {},
   "source": [
    "### \n",
    "* From the above, all columns are categorical. Shows there are anomalies since some \n",
    "  columns cant have object as a datatype i.e price etc.\n",
    "* No missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical information of numerical data\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566025c",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec14dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab97b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values and sorting the issue\n",
    "missing_perc = df.isnull().mean()*100\n",
    "missing_values = pd.DataFrame({'column_name' : df.columns, 'Missing Percentage' : missing_perc})\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286da86",
   "metadata": {},
   "source": [
    "### \n",
    "- No missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename some columns\n",
    "df1 = df1.rename(columns={'admin1': 'Province', 'admin2': 'County'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "print(*df1.columns, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19da0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes of the above columns\n",
    "for col in df1:\n",
    "    print(f'{col} : {df1[col].dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some patterns in the columns you want to convert into numerical\n",
    "num_cols = ['latitude', 'longitude', 'price', 'usdprice']\n",
    "import re\n",
    "def numeric(text):\n",
    "    text = text.lower()\n",
    "    if re.search(r'#', text):\n",
    "        text = 0\n",
    "    return text\n",
    "\n",
    "df1[num_cols] = df1[num_cols].applymap(numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert necessary columns to their respective datatypes\n",
    "num_cols = ['latitude', 'longitude', 'price', 'usdprice']\n",
    "for num in num_cols:\n",
    "    df1[num] = df1[num].astype('float64')\n",
    "    \n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4909",
   "metadata": {},
   "source": [
    "### \n",
    "- I now have a clean dataset with their corresponding data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical information of numerical data\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48392d",
   "metadata": {},
   "source": [
    "### \n",
    "- No outliers in my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce6e44",
   "metadata": {},
   "source": [
    "### 4.1 Quality mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check category \n",
    "print(*df1['category'].unique(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d391a",
   "metadata": {},
   "source": [
    "### \n",
    "- category seems to be ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check commodity \n",
    "print(*df1['commodity'].unique(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a quality mapping on commodity column\n",
    "import re\n",
    "def commodity(text):\n",
    "    text = text.lower()\n",
    "    if re.match(r'^ma', text):\n",
    "        text = 'Maize'\n",
    "    if re.match(r'^b', text):\n",
    "        text = 'Beans'\n",
    "    if re.match(r'^p', text):\n",
    "        text = 'Potatoes'\n",
    "    if re.match(r'^mi', text):\n",
    "        text = 'Milk'\n",
    "    if re.match(r'^fu', text):\n",
    "        text = 'Fuel'\n",
    "    if re.match(r'^mea', text):\n",
    "        text = 'Meat'\n",
    "    if re.match(r'^oi', text):\n",
    "        text = 'Oil'\n",
    "    if re.match(r'^on', text):\n",
    "        text = 'Onion'\n",
    "    if re.match(r'^ri', text):\n",
    "        text = 'Rice'\n",
    "    if re.match(r'^so', text):\n",
    "        text = 'Sorghum'\n",
    "    if re.match(r'^cow', text):\n",
    "        text = 'Cowpeas'\n",
    "    if re.match(r'^mil', text):\n",
    "        text = 'Millet'\n",
    "    if re.match(r'^fis', text):\n",
    "        text = 'Fish'\n",
    "    return text\n",
    "        \n",
    "df1['commodity'] = df1['commodity'].apply(commodity)\n",
    "print('Unique values in Commodity column: ')\n",
    "print('-------------------------------')\n",
    "print(*df1['commodity'].unique(), sep='\\n')\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff73c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unit column \n",
    "print(*df1['unit'].unique(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a quality mapping on unit column\n",
    "import re\n",
    "def unit(text):\n",
    "    text = text.lower()\n",
    "    if text == 'bunch' or text == 'head':\n",
    "        return text\n",
    "    text = re.sub(r'(\\d+\\s*kg|\\d+\\s*g)', '1 KG', text)\n",
    "    text = re.sub(r'(\\d+\\s*ml|\\d+\\s*l)', '1 L', text)\n",
    "    text = re.sub(r'(?<!\\d)l(?!\\w)', '1 L', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df1['unit'] = df1['unit'].apply(unit)\n",
    "print('Unique values in Unit column: ')\n",
    "print('-------------------------------')\n",
    "print(*df1['unit'].unique(), sep='\\n')\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df203aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6af29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm unique values of our columns\n",
    "excluded_cols = ['Province', 'County', 'market', 'price', 'usdprice', 'latitude', 'longitude']\n",
    "for col in df1.columns:\n",
    "    if col not in excluded_cols:\n",
    "        unique_values = \"\\n\".join(df1[col].unique().astype(str))\n",
    "        print(f'{col} : {df1[col].nunique()} : {unique_values}\\n')\n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9b822",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "- Here we will pick our essential features for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us see our columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the df1 column\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df2.drop(columns=['usdprice', 'currency'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new columns to fit our model with\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a59f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa15d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the choosen columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [column for column in df2.columns if df2[column].dtype == 'object']\n",
    "\n",
    "# binary cols are the columns with only two unique values\n",
    "binary_cols = [column for column in cat_cols if df2[column].nunique() == 3]\n",
    "print(*binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710dd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df2, columns=binary_cols, drop_first=True)\n",
    "dummied_cols = [column for column in df2.columns if column not in cat_cols and column not in binary_cols]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables to numeric values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "cat_cols = [column for column in df2.columns if df2[column].dtype == 'object']\n",
    "\n",
    "# label encode the categorical columns\n",
    "for column in cat_cols:\n",
    "    df2[column] = le.fit_transform(df2[column])\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test split\n",
    "X = df2.drop('price', axis=1)\n",
    "y = df2['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from sklearn.metrics import r2_score\n",
    "cat = CatBoostRegressor(loss_function='RMSE', n_estimators=100, learning_rate=0.05, max_depth=5)\n",
    "xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.05, max_depth=5)\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "models= [cat,linear, rf,xgb]\n",
    "\n",
    "# Fit the model to the training data\n",
    "for model in models:\n",
    "    model = model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #metric\n",
    "    mse = r2_score(y_test, y_pred) * 100\n",
    "    print(f'error for {model} is: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76d49d",
   "metadata": {},
   "source": [
    "# CONCLUSION!\n",
    "- From the trained models above,.. RandomForestRegressor, XGBRegressor and CatBoostRegressor are doint exemplary good. RandomForestRegressor tends to be best... tuning both XGBRegressor and CatBoostRegressor can be good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
